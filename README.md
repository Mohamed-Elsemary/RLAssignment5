# RLAssignment5
Here is a **clean, professional GitHub README** suitable for your **CMPS458 â€“ Assignment 5**, using **World Models** on **Space Invaders**. You can copy-paste it directly into `README.md`.

---

# World Models for Space Invaders ğŸ®ğŸ§ 

This repository implements a **model-based reinforcement learning agent** using the **World Models** framework and trains it on the **SpaceInvadersNoFrameskip-v4** Atari environment.

The project is part of **CMPS458: Reinforcement Learning (Fall 2025)**, Computer Engineering Department.

---

## ğŸ“Œ Project Overview

World Models is a **model-based RL approach** that learns a compact latent representation of the environment and uses it to plan and learn policies efficiently.

In this project, we:

* Learn a **latent world model** from pixel observations
* Train an agent to play **Space Invaders** using the learned model
* Evaluate performance and record agent gameplay
* Log experiments and metrics using **Weights & Biases (WandB)**

---

## ğŸ§  World Models Architecture

The implementation follows the original **World Models** framework:

1. **Vision Model (VAE)**

   * Compresses raw game frames into a low-dimensional latent space

2. **Memory Model (MDN-RNN)**

   * Predicts future latent states and rewards

3. **Controller (Policy Network)**

   * Selects actions based on latent states and RNN hidden states

Reference:

* Ha, D., & Schmidhuber, J. *World Models*
  [https://worldmodels.github.io/](https://worldmodels.github.io/)

---

## ğŸ® Environment

* **Gymnasium Atari**
* **SpaceInvadersNoFrameskip-v4**
* Observations: raw RGB frames
* Actions: discrete Atari action space

---

## ğŸ› ï¸ Technologies & Libraries

* Python 3.x
* PyTorch
* Gymnasium
* ALE (Atari Learning Environment)
* Weights & Biases (WandB)
* Hugging Face Hub

---

## ğŸ“ Repository Structure

```
â”œâ”€â”€ env/                # Gym environment wrappers
â”œâ”€â”€ models/             # VAE, MDN-RNN, Controller
â”œâ”€â”€ train/              # Training scripts
â”œâ”€â”€ eval/               # Evaluation & video recording
â”œâ”€â”€ configs/            # Hyperparameters
â”œâ”€â”€ results/            # Logs and saved models
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## ğŸš€ Training the Agent

1. Install dependencies:

```bash
pip install -r requirements.txt
```

2. Train the world model:

```bash
python train/train_world_model.py
```

3. Train the controller:

```bash
python train/train_controller.py
```

---

## ğŸ¥ Recording Gameplay

The trained agent is recorded using Gymnasiumâ€™s `RecordVideo` wrapper.
Videos are logged and shared via **WandB reports**.

---

## ğŸ“Š Experiment Tracking

All experiments, losses, rewards, and videos are logged using **Weights & Biases**.

---

## ğŸ§ª Results

* The agent successfully learns a compact world representation
* Performance improves as the controller leverages the learned dynamics
* Demonstrates the effectiveness of **model-based RL** on high-dimensional Atari environments

(See paper for quantitative results and charts.)

---

## ğŸ“„ Paper & Links

* ğŸ“„ Research Paper (submitted separately)
* ğŸ’» GitHub Repository: *(this repo)*
* ğŸ¥ Agent Gameplay Video: *(linked via WandB)*
* ğŸ“Š WandB Report: *(included in paper)*

---

## ğŸ“š References

1. Ha, D., & Schmidhuber, J. (2018). *World Models*.
   [https://worldmodels.github.io/](https://worldmodels.github.io/)

---

## ğŸ‘¤ Course Information

* **Course**: CMPS458 â€“ Reinforcement Learning
* **Instructor**: Dr. Ayman AboElHassan
* **Semester**: Fall 2025

---

If you want, I can:

* Make it **shorter**
* Adapt it to **Hugging Face repo**
* Add **badges** (Python, PyTorch, WandB)
* Customize it with **your team name**

Just tell me ğŸ‘
