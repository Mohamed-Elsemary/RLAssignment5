{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Welcome to Modal notebooks!\n",
        "\n",
        "Write Python code and collaborate in real time. Your code runs in Modal's\n",
        "**serverless cloud**, and anyone in the same workspace can join.\n",
        "\n",
        "This notebook comes with some common Python libraries installed. Run\n",
        "cells with `Shift+Enter`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# run this cell (in notebook, prefix with !)\n",
        "# Basic stack (adjust torch/cu version to your CUDA)\n",
        "!pip install --upgrade pip\n",
        "\n",
        "# At least these:\n",
        "!pip install gymnasium[atari] gymnasium[accept-rom-license] ale-py\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118  # or use CPU version if needed\n",
        "!pip install wandb huggingface_hub cma numpy matplotlib opencv-python tqdm\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/site-packages (24.3.1)\r\n",
            "Collecting pip\r\n",
            "  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\r\n",
            "Downloading pip-25.3-py3-none-any.whl (1.8 MB)\r\n",
            "\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m155.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hInstalling collected packages: pip\r\n",
            "  Attempting uninstall: pip\r\n",
            "    Found existing installation: pip 24.3.1\r\n",
            "    Uninstalling pip-24.3.1:\r\n",
            "      Successfully uninstalled pip-24.3.1\r\n",
            "Successfully installed pip-25.3\r\n",
            "Collecting ale-py\r\n",
            "  Downloading ale_py-0.11.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (9.0 kB)\r\n",
            "Collecting gymnasium[atari]\r\n",
            "  Downloading gymnasium-1.2.3-py3-none-any.whl.metadata (10 kB)\r\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/site-packages (from gymnasium[atari]) (2.1.2)\r\n",
            "Collecting cloudpickle>=1.2.0 (from gymnasium[atari])\r\n",
            "  Downloading cloudpickle-3.1.2-py3-none-any.whl.metadata (7.1 kB)\r\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/site-packages (from gymnasium[atari]) (4.12.2)\r\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium[atari])\r\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\r\n",
            "\u001b[33mWARNING: gymnasium 1.2.3 does not provide the extra 'accept-rom-license'\u001b[0m\u001b[33m\r\n",
            "\u001b[0mDownloading gymnasium-1.2.3-py3-none-any.whl (952 kB)\r\n",
            "\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/952.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m952.1/952.1 kB\u001b[0m \u001b[31m131.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
            "\u001b[?25hDownloading ale_py-0.11.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.1 MB)\r\n",
            "\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/5.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m131.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
            "\u001b[?25hDownloading cloudpickle-3.1.2-py3-none-any.whl (22 kB)\r\n",
            "Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\r\n",
            "Installing collected packages: farama-notifications, cloudpickle, ale-py, gymnasium\r\n",
            "\u001b[?25l\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3/4\u001b[0m [gymnasium]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3/4\u001b[0m [gymnasium]\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m4/4\u001b[0m [gymnasium]\r\n",
            "\u001b[?25h\r\u001b[1A\u001b[2KSuccessfully installed ale-py-0.11.2 cloudpickle-3.1.2 farama-notifications-0.0.4 gymnasium-1.2.3\r\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu118\r\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/site-packages (2.8.0+cu129)\r\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/site-packages (0.23.0+cu129)\r\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/site-packages (2.8.0+cu129)\r\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/site-packages (from torch) (3.13.1)\r\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/site-packages (from torch) (4.12.2)\r\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/site-packages (from torch) (70.2.0)\r\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/site-packages (from torch) (1.13.3)\r\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/site-packages (from torch) (3.3)\r\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/site-packages (from torch) (3.1.4)\r\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/site-packages (from torch) (2024.6.1)\r\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.9.86 in /usr/local/lib/python3.12/site-packages (from torch) (12.9.86)\r\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.9.79 in /usr/local/lib/python3.12/site-packages (from torch) (12.9.79)\r\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.9.79 in /usr/local/lib/python3.12/site-packages (from torch) (12.9.79)\r\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/site-packages (from torch) (9.10.2.21)\r\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.9.1.4 in /usr/local/lib/python3.12/site-packages (from torch) (12.9.1.4)\r\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.4.1.4 in /usr/local/lib/python3.12/site-packages (from torch) (11.4.1.4)\r\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.10.19 in /usr/local/lib/python3.12/site-packages (from torch) (10.3.10.19)\r\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.5.82 in /usr/local/lib/python3.12/site-packages (from torch) (11.7.5.82)\r\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.10.65 in /usr/local/lib/python3.12/site-packages (from torch) (12.5.10.65)\r\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/site-packages (from torch) (0.7.1)\r\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/site-packages (from torch) (2.27.3)\r\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.9.79 in /usr/local/lib/python3.12/site-packages (from torch) (12.9.79)\r\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.9.86 in /usr/local/lib/python3.12/site-packages (from torch) (12.9.86)\r\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.14.1.1 in /usr/local/lib/python3.12/site-packages (from torch) (1.14.1.1)\r\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/site-packages (from torch) (3.4.0)\r\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/site-packages (from torchvision) (2.1.2)\r\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/site-packages (from torchvision) (11.0.0)\r\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\r\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\r\n",
            "Collecting wandb\r\n",
            "  Downloading wandb-0.23.1-py3-none-manylinux_2_28_x86_64.whl.metadata (12 kB)\r\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/site-packages (0.34.4)\r\n",
            "Collecting cma\r\n",
            "  Downloading cma-4.4.1-py3-none-any.whl.metadata (8.7 kB)\r\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/site-packages (2.1.2)\r\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/site-packages (3.10.6)\r\n",
            "Collecting opencv-python\r\n",
            "  Downloading opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\r\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/site-packages (4.67.1)\r\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/site-packages (from wandb) (8.2.1)\r\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\r\n",
            "  Downloading gitpython-3.1.45-py3-none-any.whl.metadata (13 kB)\r\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/site-packages (from wandb) (25.0)\r\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/site-packages (from wandb) (4.4.0)\r\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/site-packages (from wandb) (5.29.2)\r\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/site-packages (from wandb) (2.11.7)\r\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/site-packages (from wandb) (6.0.2)\r\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.12/site-packages (from wandb) (2.32.5)\r\n",
            "Collecting sentry-sdk>=2.0.0 (from wandb)\r\n",
            "  Downloading sentry_sdk-2.48.0-py2.py3-none-any.whl.metadata (10 kB)\r\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/site-packages (from wandb) (4.12.2)\r\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/site-packages (from pydantic<3->wandb) (0.7.0)\r\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/site-packages (from pydantic<3->wandb) (2.33.2)\r\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/site-packages (from pydantic<3->wandb) (0.4.1)\r\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.4.3)\r\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.10)\r\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\r\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\r\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/site-packages (from huggingface_hub) (3.13.1)\r\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/site-packages (from huggingface_hub) (2024.6.1)\r\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/site-packages (from huggingface_hub) (1.1.9)\r\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/site-packages (from matplotlib) (1.3.3)\r\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/site-packages (from matplotlib) (0.12.1)\r\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/site-packages (from matplotlib) (4.59.2)\r\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/site-packages (from matplotlib) (1.4.9)\r\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/site-packages (from matplotlib) (11.0.0)\r\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/site-packages (from matplotlib) (3.2.3)\r\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\r\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\r\n",
            "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\r\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\r\n",
            "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\r\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\r\n",
            "Downloading wandb-0.23.1-py3-none-manylinux_2_28_x86_64.whl (22.9 MB)\r\n",
            "\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/22.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m128.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
            "\u001b[?25hDownloading cma-4.4.1-py3-none-any.whl (309 kB)\r\n",
            "Downloading opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (67.0 MB)\r\n",
            "\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/67.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m28.6/67.0 MB\u001b[0m \u001b[31m143.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m59.5/67.0 MB\u001b[0m \u001b[31m149.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m149.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
            "\u001b[?25hDownloading gitpython-3.1.45-py3-none-any.whl (208 kB)\r\n",
            "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\r\n",
            "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\r\n",
            "Downloading sentry_sdk-2.48.0-py2.py3-none-any.whl (414 kB)\r\n",
            "Installing collected packages: smmap, sentry-sdk, opencv-python, cma, gitdb, gitpython, wandb\r\n",
            "\u001b[?25l\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1/7\u001b[0m [sentry-sdk]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2/7\u001b[0m [opencv-python]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2/7\u001b[0m [opencv-python]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2/7\u001b[0m [opencv-python]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2/7\u001b[0m [opencv-python]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2/7\u001b[0m [opencv-python]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2/7\u001b[0m [opencv-python]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3/7\u001b[0m [cma]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m5/7\u001b[0m [gitpython]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m6/7\u001b[0m [wandb]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m6/7\u001b[0m [wandb]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m6/7\u001b[0m [wandb]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m6/7\u001b[0m [wandb]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m6/7\u001b[0m [wandb]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m6/7\u001b[0m [wandb]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m6/7\u001b[0m [wandb]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m6/7\u001b[0m [wandb]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m6/7\u001b[0m [wandb]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m6/7\u001b[0m [wandb]\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m7/7\u001b[0m [wandb]\r\n",
            "\u001b[?25h\r\u001b[1A\u001b[2KSuccessfully installed cma-4.4.1 gitdb-4.0.12 gitpython-3.1.45 opencv-python-4.12.0.88 sentry-sdk-2.48.0 smmap-5.0.2 wandb-0.23.1\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Block 1: Installation and Imports\n",
        "# Run this first to install all dependencies\n",
        "\n",
        "!pip install gymnasium[atari,accept-rom-license]\n",
        "!pip install torch torchvision\n",
        "!pip install wandb\n",
        "!pip install huggingface_hub\n",
        "!pip install ale-py\n",
        "!pip install imageio imageio-ffmpeg\n",
        "!pip install numpy pillow matplotlib\n",
        "\n",
        "# Imports\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Normal\n",
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import RecordVideo, RecordEpisodeStatistics\n",
        "import wandb\n",
        "from huggingface_hub import HfApi, upload_folder\n",
        "import os\n",
        "from collections import deque\n",
        "import random\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import ale_py\n",
        "gym.register_envs(ale_py)\n",
        "wandb.login('c63f756e765102af220cef97dd153041e4a2e751')\n",
        "print(\"\u2713 All libraries installed and imported successfully!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium[accept-rom-license,atari] in /usr/local/lib/python3.12/site-packages (1.2.3)\r\n",
            "\u001b[33mWARNING: gymnasium 1.2.3 does not provide the extra 'accept-rom-license'\u001b[0m\u001b[33m\r\n",
            "\u001b[0mRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/site-packages (from gymnasium[accept-rom-license,atari]) (2.1.2)\r\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/site-packages (from gymnasium[accept-rom-license,atari]) (3.1.2)\r\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/site-packages (from gymnasium[accept-rom-license,atari]) (4.12.2)\r\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/site-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\r\n",
            "Requirement already satisfied: ale_py>=0.9 in /usr/local/lib/python3.12/site-packages (from gymnasium[accept-rom-license,atari]) (0.11.2)\r\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/site-packages (2.8.0+cu129)\r\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/site-packages (0.23.0+cu129)\r\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/site-packages (from torch) (3.13.1)\r\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/site-packages (from torch) (4.12.2)\r\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/site-packages (from torch) (70.2.0)\r\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/site-packages (from torch) (1.13.3)\r\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/site-packages (from torch) (3.3)\r\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/site-packages (from torch) (3.1.4)\r\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/site-packages (from torch) (2024.6.1)\r\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.9.86 in /usr/local/lib/python3.12/site-packages (from torch) (12.9.86)\r\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.9.79 in /usr/local/lib/python3.12/site-packages (from torch) (12.9.79)\r\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.9.79 in /usr/local/lib/python3.12/site-packages (from torch) (12.9.79)\r\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/site-packages (from torch) (9.10.2.21)\r\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.9.1.4 in /usr/local/lib/python3.12/site-packages (from torch) (12.9.1.4)\r\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.4.1.4 in /usr/local/lib/python3.12/site-packages (from torch) (11.4.1.4)\r\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.10.19 in /usr/local/lib/python3.12/site-packages (from torch) (10.3.10.19)\r\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.5.82 in /usr/local/lib/python3.12/site-packages (from torch) (11.7.5.82)\r\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.10.65 in /usr/local/lib/python3.12/site-packages (from torch) (12.5.10.65)\r\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/site-packages (from torch) (0.7.1)\r\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/site-packages (from torch) (2.27.3)\r\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.9.79 in /usr/local/lib/python3.12/site-packages (from torch) (12.9.79)\r\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.9.86 in /usr/local/lib/python3.12/site-packages (from torch) (12.9.86)\r\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.14.1.1 in /usr/local/lib/python3.12/site-packages (from torch) (1.14.1.1)\r\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/site-packages (from torch) (3.4.0)\r\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/site-packages (from torchvision) (2.1.2)\r\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/site-packages (from torchvision) (11.0.0)\r\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\r\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\r\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/site-packages (0.23.1)\r\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/site-packages (from wandb) (8.2.1)\r\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/site-packages (from wandb) (3.1.45)\r\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/site-packages (from wandb) (25.0)\r\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/site-packages (from wandb) (4.4.0)\r\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/site-packages (from wandb) (5.29.2)\r\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/site-packages (from wandb) (2.11.7)\r\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/site-packages (from wandb) (6.0.2)\r\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.12/site-packages (from wandb) (2.32.5)\r\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/site-packages (from wandb) (2.48.0)\r\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/site-packages (from wandb) (4.12.2)\r\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/site-packages (from pydantic<3->wandb) (0.7.0)\r\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/site-packages (from pydantic<3->wandb) (2.33.2)\r\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/site-packages (from pydantic<3->wandb) (0.4.1)\r\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.4.3)\r\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.10)\r\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\r\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\r\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\r\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\r\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/site-packages (0.34.4)\r\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/site-packages (from huggingface_hub) (3.13.1)\r\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/site-packages (from huggingface_hub) (2024.6.1)\r\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/site-packages (from huggingface_hub) (25.0)\r\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/site-packages (from huggingface_hub) (6.0.2)\r\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/site-packages (from huggingface_hub) (2.32.5)\r\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/site-packages (from huggingface_hub) (4.67.1)\r\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/site-packages (from huggingface_hub) (4.12.2)\r\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/site-packages (from huggingface_hub) (1.1.9)\r\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/site-packages (from requests->huggingface_hub) (3.4.3)\r\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/site-packages (from requests->huggingface_hub) (3.10)\r\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/site-packages (from requests->huggingface_hub) (2.5.0)\r\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/site-packages (from requests->huggingface_hub) (2024.8.30)\r\n",
            "Requirement already satisfied: ale-py in /usr/local/lib/python3.12/site-packages (0.11.2)\r\n",
            "Requirement already satisfied: numpy>1.20 in /usr/local/lib/python3.12/site-packages (from ale-py) (2.1.2)\r\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.12/site-packages (2.37.0)\r\n",
            "Collecting imageio-ffmpeg\r\n",
            "  Downloading imageio_ffmpeg-0.6.0-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/site-packages (from imageio) (2.1.2)\r\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.12/site-packages (from imageio) (11.0.0)\r\n",
            "Downloading imageio_ffmpeg-0.6.0-py3-none-manylinux2014_x86_64.whl (29.5 MB)\r\n",
            "\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/29.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m29.5/29.5 MB\u001b[0m \u001b[31m168.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
            "\u001b[?25hInstalling collected packages: imageio-ffmpeg\r\n",
            "Successfully installed imageio-ffmpeg-0.6.0\r\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/site-packages (2.1.2)\r\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/site-packages (11.0.0)\r\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/site-packages (3.10.6)\r\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/site-packages (from matplotlib) (1.3.3)\r\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/site-packages (from matplotlib) (0.12.1)\r\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/site-packages (from matplotlib) (4.59.2)\r\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/site-packages (from matplotlib) (1.4.9)\r\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/site-packages (from matplotlib) (25.0)\r\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/site-packages (from matplotlib) (3.2.3)\r\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\r\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\r\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mloaya2003\u001b[0m (\u001b[33myousefyousefyousef335-cairo-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2713 All libraries installed and imported successfully!\n",
            "PyTorch version: 2.8.0+cu129\n",
            "CUDA available: True\n",
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Block 2: VAE (Vision Component - V)\n",
        "# Encodes 64x64 RGB images into a latent vector z\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, latent_dim=32):\n",
        "        super(VAE, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        \n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1),  # 64x64 -> 32x32\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),  # 32x32 -> 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),  # 16x16 -> 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),  # 8x8 -> 4x4\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        \n",
        "        # Latent space\n",
        "        self.fc_mu = nn.Linear(256 * 4 * 4, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(256 * 4 * 4, latent_dim)\n",
        "        \n",
        "        # Decoder\n",
        "        self.fc_decode = nn.Linear(latent_dim, 256 * 4 * 4)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # 4x4 -> 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),  # 8x8 -> 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),  # 16x16 -> 32x32\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1),  # 32x32 -> 64x64\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    \n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = h.view(h.size(0), -1)\n",
        "        return self.fc_mu(h), self.fc_logvar(h)\n",
        "    \n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "    \n",
        "    def decode(self, z):\n",
        "        h = self.fc_decode(z)\n",
        "        h = h.view(h.size(0), 256, 4, 4)\n",
        "        return self.decoder(h)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "def vae_loss(recon_x, x, mu, logvar):\n",
        "    # Reconstruction loss\n",
        "    recon_loss = F.mse_loss(recon_x, x, reduction='sum')\n",
        "    # KL divergence\n",
        "    kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return recon_loss + kld\n",
        "\n",
        "print(\"\u2713 VAE model defined\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2713 VAE model defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Block 3: MDN-RNN (Memory Component - M)\n",
        "# Predicts next latent state using mixture density network + LSTM\n",
        "\n",
        "class MDNRNN(nn.Module):\n",
        "    def __init__(self, latent_dim=32, action_dim=6, hidden_dim=256, num_mixtures=5):\n",
        "        super(MDNRNN, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_mixtures = num_mixtures\n",
        "        \n",
        "        # LSTM takes (z_t, a_t) as input\n",
        "        self.lstm = nn.LSTM(latent_dim + action_dim, hidden_dim, batch_first=True)\n",
        "        \n",
        "        # MDN outputs: mixture weights, means, and log variances\n",
        "        self.mdn = nn.Linear(\n",
        "            hidden_dim,\n",
        "            num_mixtures * (1 + 2 * latent_dim)\n",
        "        )\n",
        "        \n",
        "    def forward(self, z, action, hidden=None):\n",
        "        # z: (batch, seq_len, latent_dim)\n",
        "        # action: (batch, seq_len, action_dim)\n",
        "        x = torch.cat([z, action], dim=-1)\n",
        "        lstm_out, hidden = self.lstm(x, hidden)\n",
        "        \n",
        "        # Predict next latent state distribution\n",
        "        mdn_out = self.mdn(lstm_out)\n",
        "        \n",
        "        # reshape\n",
        "        mdn_out = mdn_out.view(\n",
        "            mdn_out.size(0),\n",
        "            mdn_out.size(1),\n",
        "            self.num_mixtures,\n",
        "            1 + 2 * self.latent_dim\n",
        "        )\n",
        "        \n",
        "        # split\n",
        "        pi = mdn_out[..., 0]                         # (B, T, K)\n",
        "        mu = mdn_out[..., 1:1+self.latent_dim]       # (B, T, K, Z)\n",
        "        logvar = mdn_out[..., 1+self.latent_dim:]    # (B, T, K, Z)\n",
        "        \n",
        "        # normalize mixture weights\n",
        "        pi = F.softmax(pi, dim=-1)\n",
        "        \n",
        "        return pi, mu, logvar, hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        return (torch.zeros(1, batch_size, self.hidden_dim).to(device),\n",
        "                torch.zeros(1, batch_size, self.hidden_dim).to(device))\n",
        "\n",
        "def mdn_loss(pi, mu, logvar, target):\n",
        "    \"\"\"\n",
        "    pi:     (B, T, K)\n",
        "    mu:     (B, T, K, Z)\n",
        "    logvar: (B, T, K, Z)\n",
        "    target: (B, T, Z)\n",
        "    \"\"\"\n",
        "\n",
        "    # Expand target for mixture dimension\n",
        "    target = target.unsqueeze(2)  # (B, T, 1, Z)\n",
        "\n",
        "    # Gaussian log-likelihood\n",
        "    var = torch.exp(logvar)\n",
        "    log_prob = -0.5 * (\n",
        "        torch.log(2 * np.pi * var) +\n",
        "        (target - mu) ** 2 / var\n",
        "    )\n",
        "    log_prob = log_prob.sum(dim=-1)  # (B, T, K)\n",
        "\n",
        "    # Add log mixture weights\n",
        "    log_prob = log_prob + torch.log(pi + 1e-8)\n",
        "\n",
        "    # Log-sum-exp over mixtures (STABLE)\n",
        "    log_prob = torch.logsumexp(log_prob, dim=2)  # (B, T)\n",
        "\n",
        "    # Negative log likelihood\n",
        "    return -log_prob.mean()\n",
        "print(\"\u2713 MDN-RNN model defined\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2713 MDN-RNN model defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Block 4: Controller (C)\n",
        "# Simple linear policy that maps [z, h] to action\n",
        "\n",
        "class Controller(nn.Module):\n",
        "    def __init__(self, latent_dim=32, hidden_dim=256, action_dim=6):\n",
        "        super(Controller, self).__init__()\n",
        "        self.fc = nn.Linear(latent_dim + hidden_dim, action_dim)\n",
        "    \n",
        "    def forward(self, z, h):\n",
        "        # z: latent state from VAE\n",
        "        # h: hidden state from LSTM\n",
        "        x = torch.cat([z, h], dim=-1)\n",
        "        return self.fc(x)\n",
        "\n",
        "# CMA-ES optimizer for evolution strategies\n",
        "class CMAES:\n",
        "    def __init__(self, num_params, population_size=64, sigma=0.5):\n",
        "        self.num_params = num_params\n",
        "        self.population_size = population_size\n",
        "        self.sigma = sigma\n",
        "        self.mean = np.zeros(num_params)\n",
        "        \n",
        "    def ask(self):\n",
        "        # Generate population\n",
        "        return [self.mean + self.sigma * np.random.randn(self.num_params) \n",
        "                for _ in range(self.population_size)]\n",
        "    \n",
        "    def tell(self, solutions, rewards):\n",
        "        # Update distribution based on top performers\n",
        "        idx = np.argsort(rewards)[::-1]\n",
        "        elite_size = self.population_size // 4\n",
        "        elite_params = [solutions[i] for i in idx[:elite_size]]\n",
        "        \n",
        "        self.mean = np.mean(elite_params, axis=0)\n",
        "        self.sigma = np.std(elite_params)\n",
        "\n",
        "print(\"\u2713 Controller model defined\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2713 Controller model defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Block 5: Data Collection and Preprocessing\n",
        "\n",
        "class AtariPreprocessing:\n",
        "    \"\"\"Preprocess Atari frames to 64x64 RGB\"\"\"\n",
        "    def __init__(self, size=64):\n",
        "        self.size = size\n",
        "    \n",
        "    def process(self, frame):\n",
        "        # Convert to PIL Image\n",
        "        img = Image.fromarray(frame)\n",
        "        # Resize to 64x64\n",
        "        img = img.resize((self.size, self.size), Image.BILINEAR)\n",
        "        # Convert to numpy array and normalize\n",
        "        img = np.array(img).astype(np.float32) / 255.0\n",
        "        # Transpose to (C, H, W)\n",
        "        return img.transpose(2, 0, 1)\n",
        "\n",
        "def collect_random_episodes(env_name, num_episodes=100, max_steps=1000):\n",
        "    \"\"\"Collect random rollouts for VAE training\"\"\"\n",
        "    env = gym.make(env_name)\n",
        "    preprocessor = AtariPreprocessing()\n",
        "    episodes = []\n",
        "    \n",
        "    for ep in range(num_episodes):\n",
        "        obs, _ = env.reset()\n",
        "        episode = {'observations': [], 'actions': [], 'rewards': []}\n",
        "        \n",
        "        for step in range(max_steps):\n",
        "            # Preprocess observation\n",
        "            processed_obs = preprocessor.process(obs)\n",
        "            episode['observations'].append(processed_obs)\n",
        "            \n",
        "            # Random action\n",
        "            action = env.action_space.sample()\n",
        "            episode['actions'].append(action)\n",
        "            \n",
        "            obs, reward, terminated, truncated, _ = env.step(action)\n",
        "            episode['rewards'].append(reward)\n",
        "            \n",
        "            if terminated or truncated:\n",
        "                break\n",
        "        \n",
        "        episodes.append(episode)\n",
        "        if (ep + 1) % 10 == 0:\n",
        "            print(f\"Collected {ep + 1}/{num_episodes} episodes\")\n",
        "    \n",
        "    env.close()\n",
        "    return episodes\n",
        "\n",
        "class ReplayBuffer:\n",
        "    \"\"\"Store and sample experiences\"\"\"\n",
        "    def __init__(self, capacity=10000):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "    \n",
        "    def push(self, obs, action, next_obs, reward):\n",
        "        self.buffer.append((obs, action, next_obs, reward))\n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        obs, actions, next_obs, rewards = zip(*batch)\n",
        "        return (torch.FloatTensor(np.array(obs)).to(device),\n",
        "                torch.FloatTensor(np.array(actions)).to(device),\n",
        "                torch.FloatTensor(np.array(next_obs)).to(device),\n",
        "                torch.FloatTensor(np.array(rewards)).to(device))\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "print(\"\u2713 Data collection utilities defined\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2713 Data collection utilities defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Block 6: Hyperparameters and WandB Configuration\n",
        "\n",
        "# Hyperparameters\n",
        "config = {\n",
        "    # Environment\n",
        "    'env_name': 'SpaceInvadersNoFrameskip-v4',\n",
        "    'max_episode_steps': 1000,\n",
        "    'max_episode_steps_for_videos': 10000,\n",
        "    \n",
        "    # Model dimensions\n",
        "    'latent_dim': 32,\n",
        "    'hidden_dim': 256,\n",
        "    'num_mixtures': 5,\n",
        "    'action_dim': 6,  # Space Invaders has 6 actions\n",
        "    \n",
        "    # VAE training\n",
        "    'vae_epochs': 10,\n",
        "    'vae_batch_size': 32,\n",
        "    'vae_lr': 0.0001,\n",
        "    'num_random_episodes': 100,\n",
        "    \n",
        "    # MDN-RNN training\n",
        "    'rnn_epochs': 20,\n",
        "    'rnn_batch_size': 16,\n",
        "    'rnn_lr': 0.0001,\n",
        "    'sequence_length': 64,\n",
        "    \n",
        "    # Controller training (CMA-ES)\n",
        "    'population_size': 64,\n",
        "    'num_generations': 10,\n",
        "    'sigma': 0.5,\n",
        "    \n",
        "    # Evaluation\n",
        "    'eval_episodes': 10,\n",
        "    'record_video_every': 10,\n",
        "    \n",
        "    # Device\n",
        "    'device': str(device),\n",
        "}\n",
        "\n",
        "# Initialize WandB\n",
        "def init_wandb(project_name=\"world-models-spaceinvaders\"):\n",
        "    \"\"\"Initialize Weights & Biases logging\"\"\"\n",
        "    wandb.login('c63f756e765102af220cef97dd153041e4a2e751')  # You'll need to enter your API key\n",
        "    \n",
        "    run = wandb.init(\n",
        "        project=project_name,\n",
        "        config=config,\n",
        "        name=f\"world-models-{config['env_name']}\",\n",
        "        tags=[\"world-models\", \"space-invaders\", \"atari\"],\n",
        "    )\n",
        "    \n",
        "    print(f\"\u2713 WandB initialized: {run.url}\")\n",
        "    return run\n",
        "\n",
        "# Uncomment to initialize WandB (requires API key)\n",
        "wandb_run = init_wandb()\n",
        "\n",
        "print(\"\u2713 Hyperparameters configured\")\n",
        "print(f\"Configuration: {config}\")"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "import gymnasium as gym\n",
        "print([env for env in gym.envs.registry if \"Space\" in env])\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['SpaceInvaders-v0', 'SpaceInvaders-v4', 'SpaceInvadersNoFrameskip-v0', 'SpaceInvadersNoFrameskip-v4', 'ALE/SpaceInvaders-v5', 'ALE/SpaceWar-v5']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Block 7: Train VAE (Vision Model)\n",
        "\n",
        "def train_vae(episodes, config):\n",
        "    \"\"\"Train VAE on collected observations\"\"\"\n",
        "    vae = VAE(latent_dim=config['latent_dim']).to(device)\n",
        "    optimizer = optim.Adam(vae.parameters(), lr=config['vae_lr'])\n",
        "    \n",
        "    # Prepare training data\n",
        "    all_observations = []\n",
        "    for ep in episodes:\n",
        "        all_observations.extend(ep['observations'])\n",
        "    \n",
        "    observations = torch.FloatTensor(np.array(all_observations)).to(device)\n",
        "    print(f\"Training VAE on {len(observations)} observations\")\n",
        "    \n",
        "    # Training loop\n",
        "    vae.train()\n",
        "    for epoch in range(config['vae_epochs']):\n",
        "        total_loss = 0\n",
        "        num_batches = 0\n",
        "        \n",
        "        # Shuffle data\n",
        "        indices = torch.randperm(len(observations))\n",
        "        \n",
        "        for i in range(0, len(observations), config['vae_batch_size']):\n",
        "            batch_idx = indices[i:i + config['vae_batch_size']]\n",
        "            batch = observations[batch_idx]\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            recon, mu, logvar = vae(batch)\n",
        "            loss = vae_loss(recon, batch, mu, logvar)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "        \n",
        "        avg_loss = total_loss / num_batches\n",
        "        print(f\"Epoch {epoch+1}/{config['vae_epochs']}, Loss: {avg_loss:.4f}\")\n",
        "        \n",
        "        # Log to WandB\n",
        "        if wandb.run is not None:\n",
        "            wandb.log({\n",
        "                'vae_loss': avg_loss,\n",
        "                'vae_epoch': epoch\n",
        "            })\n",
        "    \n",
        "    print(\"\u2713 VAE training complete\")\n",
        "    return vae\n",
        "\n",
        "# Collect data and train VAE\n",
        "print(\"Collecting random episodes...\")\n",
        "episodes = collect_random_episodes(\n",
        "    config['env_name'], \n",
        "    num_episodes=config['num_random_episodes'],\n",
        "    max_steps=config['max_episode_steps']\n",
        ")\n",
        "\n",
        "print(\"\\nTraining VAE...\")\n",
        "vae_model = train_vae(episodes, config)\n",
        "\n",
        "# Save VAE\n",
        "torch.save(vae_model.state_dict(), 'vae_model.pt')\n",
        "print(\"\u2713 VAE model saved to 'vae_model.pt'\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A.L.E: Arcade Learning Environment (version 0.11.2+ecc1138)\n",
            "[Powered by Stella]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting random episodes...\n",
            "Collected 10/100 episodes\n",
            "Collected 20/100 episodes\n",
            "Collected 30/100 episodes\n",
            "Collected 40/100 episodes\n",
            "Collected 50/100 episodes\n",
            "Collected 60/100 episodes\n",
            "Collected 70/100 episodes\n",
            "Collected 80/100 episodes\n",
            "Collected 90/100 episodes\n",
            "Collected 100/100 episodes\n",
            "\n",
            "Training VAE...\n",
            "Training VAE on 100000 observations\n",
            "Epoch 1/10, Loss: 2836.3719\n",
            "Epoch 2/10, Loss: 368.8099\n",
            "Epoch 3/10, Loss: 268.8175\n",
            "Epoch 4/10, Loss: 256.3015\n",
            "Epoch 5/10, Loss: 248.3833\n",
            "Epoch 6/10, Loss: 240.3462\n",
            "Epoch 7/10, Loss: 236.1517\n",
            "Epoch 8/10, Loss: 233.0855\n",
            "Epoch 9/10, Loss: 230.6106\n",
            "Epoch 10/10, Loss: 228.8341\n",
            "\u2713 VAE training complete\n",
            "\u2713 VAE model saved to 'vae_model.pt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Block 8: Train MDN-RNN (Memory Model)\n",
        "\n",
        "def prepare_rnn_data(episodes, vae_model, config):\n",
        "    \"\"\"Encode observations to latent space and prepare sequences\"\"\"\n",
        "    vae_model.eval()\n",
        "    sequences = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for ep in episodes:\n",
        "            if len(ep['observations']) < config['sequence_length'] + 1:\n",
        "                continue\n",
        "            \n",
        "            # Encode all observations\n",
        "            obs = torch.FloatTensor(np.array(ep['observations'])).to(device)\n",
        "            mu, _ = vae_model.encode(obs)\n",
        "            \n",
        "            # Create sequences\n",
        "            for i in range(len(mu) - config['sequence_length']):\n",
        "                z_seq = mu[i:i + config['sequence_length']]\n",
        "                z_next = mu[i + 1:i + config['sequence_length'] + 1]\n",
        "                actions = ep['actions'][i:i + config['sequence_length']]\n",
        "                \n",
        "                # One-hot encode actions\n",
        "                actions_onehot = np.zeros((config['sequence_length'], config['action_dim']))\n",
        "                for j, a in enumerate(actions):\n",
        "                    actions_onehot[j, a] = 1.0\n",
        "                \n",
        "                sequences.append({\n",
        "                    'z': z_seq.cpu().numpy(),\n",
        "                    'z_next': z_next.cpu().numpy(),\n",
        "                    'actions': actions_onehot\n",
        "                })\n",
        "    \n",
        "    print(f\"Prepared {len(sequences)} training sequences\")\n",
        "    return sequences\n",
        "\n",
        "def train_mdnrnn(sequences, config):\n",
        "    \"\"\"Train MDN-RNN on latent sequences\"\"\"\n",
        "    mdnrnn = MDNRNN(\n",
        "        latent_dim=config['latent_dim'],\n",
        "        action_dim=config['action_dim'],\n",
        "        hidden_dim=config['hidden_dim'],\n",
        "        num_mixtures=config['num_mixtures']\n",
        "    ).to(device)\n",
        "    \n",
        "    optimizer = optim.Adam(mdnrnn.parameters(), lr=config['rnn_lr'])\n",
        "    \n",
        "    mdnrnn.train()\n",
        "    for epoch in range(config['rnn_epochs']):\n",
        "        total_loss = 0\n",
        "        num_batches = 0\n",
        "        \n",
        "        # Shuffle sequences\n",
        "        random.shuffle(sequences)\n",
        "        \n",
        "        for i in range(0, len(sequences), config['rnn_batch_size']):\n",
        "            batch = sequences[i:i + config['rnn_batch_size']]\n",
        "            \n",
        "            # Prepare batch tensors\n",
        "            z = torch.FloatTensor([s['z'] for s in batch]).to(device)\n",
        "            z_next = torch.FloatTensor([s['z_next'] for s in batch]).to(device)\n",
        "            actions = torch.FloatTensor([s['actions'] for s in batch]).to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            pi, mu, logvar, _ = mdnrnn(z, actions)\n",
        "            loss = mdn_loss(pi, mu, logvar, z_next)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "        \n",
        "        avg_loss = total_loss / num_batches\n",
        "        print(f\"Epoch {epoch+1}/{config['rnn_epochs']}, Loss: {avg_loss:.4f}\")\n",
        "        \n",
        "        # Log to WandB\n",
        "        if wandb.run is not None:\n",
        "            wandb.log({\n",
        "                'rnn_loss': avg_loss,\n",
        "                'rnn_epoch': epoch\n",
        "            })\n",
        "    \n",
        "    print(\"\u2713 MDN-RNN training complete\")\n",
        "    return mdnrnn\n",
        "\n",
        "# Prepare sequences and train RNN\n",
        "print(\"Preparing RNN training data...\")\n",
        "rnn_sequences = prepare_rnn_data(episodes, vae_model, config)\n",
        "\n",
        "print(\"\\nTraining MDN-RNN...\")\n",
        "rnn_model = train_mdnrnn(rnn_sequences, config)\n",
        "\n",
        "# Save MDN-RNN\n",
        "torch.save(rnn_model.state_dict(), 'mdnrnn_model.pt')\n",
        "print(\"\u2713 MDN-RNN model saved to 'mdnrnn_model.pt'\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing RNN training data...\n",
            "Prepared 93600 training sequences\n",
            "\n",
            "Training MDN-RNN...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_341/944176263.py:60: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
            "  z = torch.FloatTensor([s['z'] for s in batch]).to(device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: -89.3372\n",
            "Epoch 2/20, Loss: -119.7274\n",
            "Epoch 3/20, Loss: -133.0856\n",
            "Epoch 4/20, Loss: -142.2652\n",
            "Epoch 5/20, Loss: -146.6168\n",
            "Epoch 6/20, Loss: -149.9334\n",
            "Epoch 7/20, Loss: -152.1637\n",
            "Epoch 8/20, Loss: -153.9065\n",
            "Epoch 9/20, Loss: -155.2766\n",
            "Epoch 10/20, Loss: -156.3323\n",
            "Epoch 11/20, Loss: -157.2448\n",
            "Epoch 12/20, Loss: -157.5669\n",
            "Epoch 13/20, Loss: -158.6652\n",
            "Epoch 14/20, Loss: -142.2032\n",
            "Epoch 15/20, Loss: -130.6961\n",
            "Epoch 16/20, Loss: -144.2273\n",
            "Epoch 17/20, Loss: -149.6112\n",
            "Epoch 18/20, Loss: -152.9538\n",
            "Epoch 19/20, Loss: -155.2308\n",
            "Epoch 20/20, Loss: -156.5111\n",
            "\u2713 MDN-RNN training complete\n",
            "\u2713 MDN-RNN model saved to 'mdnrnn_model.pt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Block 9: Train Controller using CMA-ES\n",
        "\n",
        "def set_controller_params(controller, params):\n",
        "    \"\"\"Set controller parameters from flat array\"\"\"\n",
        "    state_dict = controller.state_dict()\n",
        "    pointer = 0\n",
        "    \n",
        "    for key in state_dict.keys():\n",
        "        param_shape = state_dict[key].shape\n",
        "        param_size = np.prod(param_shape)\n",
        "        state_dict[key] = torch.FloatTensor(\n",
        "            params[pointer:pointer + param_size].reshape(param_shape)\n",
        "        )\n",
        "        pointer += param_size\n",
        "    \n",
        "    controller.load_state_dict(state_dict)\n",
        "\n",
        "def evaluate_controller(controller, vae_model, rnn_model, env_name, num_episodes=5):\n",
        "    \"\"\"Evaluate controller performance\"\"\"\n",
        "    env = gym.make(env_name)\n",
        "    preprocessor = AtariPreprocessing()\n",
        "    total_reward = 0\n",
        "    \n",
        "    vae_model.eval()\n",
        "    rnn_model.eval()\n",
        "    controller.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_episodes):\n",
        "            obs, _ = env.reset()\n",
        "            hidden = rnn_model.init_hidden(1)\n",
        "            episode_reward = 0\n",
        "            \n",
        "            for _ in range(config['max_episode_steps']):\n",
        "                # Preprocess and encode observation\n",
        "                processed_obs = preprocessor.process(obs)\n",
        "                obs_tensor = torch.FloatTensor(processed_obs).unsqueeze(0).to(device)\n",
        "                z, _ = vae_model.encode(obs_tensor)\n",
        "                \n",
        "                # Get action from controller\n",
        "                h = hidden[0].squeeze(0)  # Extract hidden state\n",
        "                action_logits = controller(z, h)\n",
        "                action = torch.argmax(action_logits, dim=-1).item()\n",
        "                \n",
        "                # Step environment\n",
        "                obs, reward, terminated, truncated, _ = env.step(action)\n",
        "                episode_reward += reward\n",
        "                \n",
        "                # Update RNN hidden state\n",
        "                action_onehot = torch.zeros(1, 1, config['action_dim']).to(device)\n",
        "                action_onehot[0, 0, action] = 1.0\n",
        "                _, _, _, hidden = rnn_model(z.unsqueeze(1), action_onehot, hidden)\n",
        "                \n",
        "                if terminated or truncated:\n",
        "                    break\n",
        "            \n",
        "            total_reward += episode_reward\n",
        "    \n",
        "    env.close()\n",
        "    return total_reward / num_episodes\n",
        "\n",
        "def train_controller_cmaes(vae_model, rnn_model, config):\n",
        "    \"\"\"Train controller using CMA-ES evolutionary strategy\"\"\"\n",
        "    controller = Controller(\n",
        "        latent_dim=config['latent_dim'],\n",
        "        hidden_dim=config['hidden_dim'],\n",
        "        action_dim=config['action_dim']\n",
        "    ).to(device)\n",
        "    \n",
        "    # Count parameters\n",
        "    num_params = sum(p.numel() for p in controller.parameters())\n",
        "    print(f\"Controller has {num_params} parameters\")\n",
        "    \n",
        "    # Initialize CMA-ES\n",
        "    cmaes = CMAES(num_params, population_size=config['population_size'], sigma=config['sigma'])\n",
        "    \n",
        "    best_reward = float('-inf')\n",
        "    best_params = None\n",
        "    \n",
        "    for generation in range(config['num_generations']):\n",
        "        # Generate population\n",
        "        solutions = cmaes.ask()\n",
        "        rewards = []\n",
        "        \n",
        "        # Evaluate each solution\n",
        "        for i, params in enumerate(solutions):\n",
        "            set_controller_params(controller, params)\n",
        "            reward = evaluate_controller(controller, vae_model, rnn_model, config['env_name'])\n",
        "            rewards.append(reward)\n",
        "            \n",
        "            if reward > best_reward:\n",
        "                best_reward = reward\n",
        "                best_params = params\n",
        "        \n",
        "        # Update distribution\n",
        "        cmaes.tell(solutions, rewards)\n",
        "        \n",
        "        avg_reward = np.mean(rewards)\n",
        "        print(f\"Generation {generation+1}/{config['num_generations']}, \"\n",
        "              f\"Avg Reward: {avg_reward:.2f}, Best: {best_reward:.2f}\")\n",
        "        \n",
        "        # Log to WandB\n",
        "        if wandb.run is not None:\n",
        "            wandb.log({\n",
        "                'generation': generation,\n",
        "                'avg_reward': avg_reward,\n",
        "                'best_reward': best_reward,\n",
        "                'max_generation_reward': max(rewards),\n",
        "                'min_generation_reward': min(rewards)\n",
        "            })\n",
        "    \n",
        "    # Set best parameters\n",
        "    set_controller_params(controller, best_params)\n",
        "    print(f\"\u2713 Controller training complete. Best reward: {best_reward:.2f}\")\n",
        "    return controller\n",
        "\n",
        "# Train controller\n",
        "print(\"Training controller with CMA-ES...\")\n",
        "controller_model = train_controller_cmaes(vae_model, rnn_model, config)\n",
        "\n",
        "# Save controller\n",
        "torch.save(controller_model.state_dict(), 'controller_model.pt')\n",
        "print(\"\u2713 Controller model saved to 'controller_model.pt'\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training controller with CMA-ES...\n",
            "Controller has 1734 parameters\n",
            "Generation 1/10, Avg Reward: 44.22, Best: 135.00\n",
            "Generation 2/10, Avg Reward: 89.38, Best: 135.00\n",
            "Generation 3/10, Avg Reward: 89.30, Best: 135.00\n",
            "Generation 4/10, Avg Reward: 119.61, Best: 135.00\n",
            "Generation 5/10, Avg Reward: 110.39, Best: 195.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "!pip install \"gymnasium[other]\""
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Block 10: Record Video and Final Evaluation\n",
        "\n",
        "def record_agent(vae_model, rnn_model, controller_model, env_name, \n",
        "                 video_folder='videos', num_episodes=5):\n",
        "    \"\"\"Record trained agent playing the game\"\"\"\n",
        "    os.makedirs(video_folder, exist_ok=True)\n",
        "    \n",
        "    # Create environment with video recording\n",
        "    env = gym.make(env_name, render_mode='rgb_array')\n",
        "    env = RecordVideo(\n",
        "        env, \n",
        "        video_folder=video_folder,\n",
        "        episode_trigger=lambda x: True,  # Record all episodes\n",
        "        name_prefix='world-models2'\n",
        "    )\n",
        "    env = RecordEpisodeStatistics(env)\n",
        "    \n",
        "    preprocessor = AtariPreprocessing()\n",
        "    \n",
        "    vae_model.eval()\n",
        "    rnn_model.eval()\n",
        "    controller_model.eval()\n",
        "    \n",
        "    episode_rewards = []\n",
        "    episode_lengths = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for ep in range(num_episodes):\n",
        "            obs, _ = env.reset()\n",
        "            hidden = rnn_model.init_hidden(1)\n",
        "            episode_reward = 0\n",
        "            steps = 0\n",
        "            \n",
        "            for _ in range(config['max_episode_steps_for_videos']):\n",
        "                # Preprocess and encode\n",
        "                processed_obs = preprocessor.process(obs)\n",
        "                obs_tensor = torch.FloatTensor(processed_obs).unsqueeze(0).to(device)\n",
        "                z, _ = vae_model.encode(obs_tensor)\n",
        "                \n",
        "                # Get action\n",
        "                h = hidden[0].squeeze(0)\n",
        "                action_logits = controller_model(z, h)\n",
        "                action = torch.argmax(action_logits, dim=-1).item()\n",
        "                \n",
        "                # Step environment\n",
        "                obs, reward, terminated, truncated, info = env.step(action)\n",
        "                episode_reward += reward\n",
        "                steps += 1\n",
        "                \n",
        "                # Update hidden state\n",
        "                action_onehot = torch.zeros(1, 1, config['action_dim']).to(device)\n",
        "                action_onehot[0, 0, action] = 1.0\n",
        "                _, _, _, hidden = rnn_model(z.unsqueeze(1), action_onehot, hidden)\n",
        "                \n",
        "                if terminated or truncated:\n",
        "                    break\n",
        "            \n",
        "            episode_rewards.append(episode_reward)\n",
        "            episode_lengths.append(steps)\n",
        "            print(f\"Episode {ep+1}: Reward={episode_reward:.2f}, Length={steps}\")\n",
        "    \n",
        "    env.close()\n",
        "    \n",
        "    # Summary statistics\n",
        "    stats = {\n",
        "        'mean_reward': np.mean(episode_rewards),\n",
        "        'std_reward': np.std(episode_rewards),\n",
        "        'mean_length': np.mean(episode_lengths),\n",
        "        'min_reward': np.min(episode_rewards),\n",
        "        'max_reward': np.max(episode_rewards)\n",
        "    }\n",
        "    \n",
        "    print(\"\\n=== Evaluation Results ===\")\n",
        "    for key, value in stats.items():\n",
        "        print(f\"{key}: {value:.2f}\")\n",
        "    \n",
        "    # Log to WandB\n",
        "    if wandb.run is not None:\n",
        "        wandb.log(stats)\n",
        "        \n",
        "        # Upload videos\n",
        "        for video_file in os.listdir(video_folder):\n",
        "            if video_file.endswith('.mp4'):\n",
        "                video_path = os.path.join(video_folder, video_file)\n",
        "                wandb.log({\"video\": wandb.Video(video_path)})\n",
        "    \n",
        "    return stats\n",
        "\n",
        "# Record videos and evaluate\n",
        "print(\"Recording agent gameplay...\")\n",
        "eval_stats = record_agent(\n",
        "    vae_model, \n",
        "    rnn_model, \n",
        "    controller_model,\n",
        "    config['env_name'],\n",
        "    num_episodes=config['eval_episodes']\n",
        ")\n",
        "\n",
        "print(\"\\n\u2713 Video recording complete! Check the 'videos' folder.\")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "!zip -r videos.zip videos"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Block 11: Publish to Hugging Face Hub\n",
        "\n",
        "import json\n",
        "from huggingface_hub import HfApi, create_repo, upload_folder\n",
        "from pathlib import Path\n",
        "\n",
        "def create_model_card(config, eval_stats):\n",
        "    \"\"\"Create a model card for Hugging Face\"\"\"\n",
        "    model_card = f\"\"\"---\n",
        "tags:\n",
        "- reinforcement-learning\n",
        "- world-models\n",
        "- atari\n",
        "- space-invaders\n",
        "- deep-learning\n",
        "library_name: pytorch\n",
        "---\n",
        "\n",
        "# World Models for Space Invaders\n",
        "\n",
        "This is a World Models agent trained on the `SpaceInvadersNoFrameskip-v4` environment.\n",
        "\n",
        "## Model Description\n",
        "\n",
        "World Models is a model-based reinforcement learning approach that learns a compressed representation \n",
        "of the environment and trains a controller to maximize reward in the learned model.\n",
        "\n",
        "The architecture consists of three components:\n",
        "- **V (Vision)**: Variational Autoencoder that compresses 64x64 RGB frames to {config['latent_dim']}-dimensional latent vectors\n",
        "- **M (Memory)**: MDN-RNN that predicts the next latent state given current state and action\n",
        "- **C (Controller)**: Linear policy trained with CMA-ES evolution strategy\n",
        "\n",
        "## Training Details\n",
        "\n",
        "### Hyperparameters\n",
        "- VAE Latent Dimension: {config['latent_dim']}\n",
        "- RNN Hidden Dimension: {config['hidden_dim']}\n",
        "- Number of Gaussian Mixtures: {config['num_mixtures']}\n",
        "- Population Size (CMA-ES): {config['population_size']}\n",
        "- Training Episodes: {config['num_random_episodes']}\n",
        "- VAE Epochs: {config['vae_epochs']}\n",
        "- RNN Epochs: {config['rnn_epochs']}\n",
        "- Controller Generations: {config['num_generations']}\n",
        "\n",
        "## Evaluation Results\n",
        "\n",
        "- **Mean Reward**: {eval_stats['mean_reward']:.2f} \u00b1 {eval_stats['std_reward']:.2f}\n",
        "- **Max Reward**: {eval_stats['max_reward']:.2f}\n",
        "- **Mean Episode Length**: {eval_stats['mean_length']:.2f}\n",
        "\n",
        "## Usage\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import gymnasium as gym\n",
        "\n",
        "# Load models\n",
        "vae = VAE(latent_dim={config['latent_dim']})\n",
        "vae.load_state_dict(torch.load('vae_model.pt'))\n",
        "\n",
        "rnn = MDNRNN(latent_dim={config['latent_dim']}, action_dim={config['action_dim']})\n",
        "rnn.load_state_dict(torch.load('mdnrnn_model.pt'))\n",
        "\n",
        "controller = Controller(latent_dim={config['latent_dim']}, hidden_dim={config['hidden_dim']})\n",
        "controller.load_state_dict(torch.load('controller_model.pt'))\n",
        "\n",
        "# Run agent\n",
        "env = gym.make('SpaceInvadersNoFrameskip-v4')\n",
        "# ... (see repository for full inference code)\n",
        "```\n",
        "\n",
        "## References\n",
        "\n",
        "- Paper: [World Models (Ha & Schmidhuber, 2018)](https://worldmodels.github.io/)\n",
        "- Code: Based on the original World Models implementation\n",
        "\n",
        "## Citation\n",
        "\n",
        "```bibtex\n",
        "@article{{ha2018worldmodels,\n",
        "  title={{World Models}},\n",
        "  author={{Ha, David and Schmidhuber, J{{\\\\\"u}}rgen}},\n",
        "  journal={{arXiv preprint arXiv:1803.10122}},\n",
        "  year={{2018}}\n",
        "}}\n",
        "```\n",
        "\"\"\"\n",
        "    return model_card\n",
        "\n",
        "def publish_to_huggingface(repo_name, config, eval_stats, hf_token=None):\n",
        "    \"\"\"\n",
        "    Publish model to Hugging Face Hub\n",
        "    \n",
        "    Args:\n",
        "        repo_name: Name for the repository (e.g., \"username/world-models-spaceinvaders\")\n",
        "        config: Configuration dictionary\n",
        "        eval_stats: Evaluation statistics\n",
        "        hf_token: Hugging Face API token (optional if already logged in)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Create local directory for repo\n",
        "    repo_dir = Path(\"hf_repo\")\n",
        "    repo_dir.mkdir(exist_ok=True)\n",
        "    \n",
        "    # Save models\n",
        "    print(\"Preparing files for upload...\")\n",
        "    torch.save(vae_model.state_dict(), repo_dir / \"vae_model.pt\")\n",
        "    torch.save(rnn_model.state_dict(), repo_dir / \"mdnrnn_model.pt\")\n",
        "    torch.save(controller_model.state_dict(), repo_dir / \"controller_model.pt\")\n",
        "    \n",
        "    # Save config\n",
        "    with open(repo_dir / \"config.json\", \"w\") as f:\n",
        "        json.dump(config, f, indent=2)\n",
        "    \n",
        "    # Save eval stats\n",
        "    with open(repo_dir / \"eval_stats.json\", \"w\") as f:\n",
        "        json.dump(eval_stats, f, indent=2)\n",
        "    \n",
        "    # Create model card\n",
        "    model_card = create_model_card(config, eval_stats)\n",
        "    with open(repo_dir / \"README.md\", \"w\") as f:\n",
        "        f.write(model_card)\n",
        "    \n",
        "    # Copy a sample video if available\n",
        "    video_files = list(Path(\"videos\").glob(\"*.mp4\"))\n",
        "    if video_files:\n",
        "        import shutil\n",
        "        shutil.copy(video_files[0], repo_dir / \"sample_gameplay.mp4\")\n",
        "        print(f\"Added sample video: {video_files[0].name}\")\n",
        "    \n",
        "    # Initialize Hugging Face API\n",
        "    api = HfApi()\n",
        "    \n",
        "    if hf_token:\n",
        "        api.token = hf_token\n",
        "    \n",
        "    # Create repository\n",
        "    try:\n",
        "        print(f\"Creating repository: {repo_name}\")\n",
        "        create_repo(\n",
        "            repo_id=repo_name,\n",
        "            repo_type=\"model\",\n",
        "            exist_ok=True,\n",
        "            token=hf_token\n",
        "        )\n",
        "        print(\"\u2713 Repository created\")\n",
        "    except Exception as e:\n",
        "        print(f\"Repository might already exist: {e}\")\n",
        "    \n",
        "    # Upload files\n",
        "    print(\"Uploading files to Hugging Face...\")\n",
        "    upload_folder(\n",
        "        folder_path=str(repo_dir),\n",
        "        repo_id=repo_name,\n",
        "        repo_type=\"model\",\n",
        "        token=hf_token,\n",
        "        commit_message=\"Upload World Models for Space Invaders\"\n",
        "    )\n",
        "    \n",
        "    print(f\"\u2713 Model published successfully!\")\n",
        "    print(f\"View at: https://huggingface.co/{repo_name}\")\n",
        "    \n",
        "    return f\"https://huggingface.co/{repo_name}\"\n",
        "\n",
        "# Example usage (uncomment and modify):\n",
        "HF_TOKEN = \"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\"  # Get from https://huggingface.co/settings/tokens\n",
        "REPO_NAME = \"loayahmed123/world-models-spaceinvadersW\"\n",
        "# \n",
        "url = publish_to_huggingface(REPO_NAME, config, eval_stats, HF_TOKEN)\n",
        "print(f\"\\nModel URL: {url}\")\n",
        "\n",
        "print(\"\u2713 Hugging Face publishing utilities ready\")\n",
        "print(\"\\nTo publish your model:\")\n",
        "print(\"1. Get your HF token from: https://huggingface.co/settings/tokens\")\n",
        "print(\"2. Set HF_TOKEN and REPO_NAME variables above\")\n",
        "print(\"3. Uncomment and run the publish_to_huggingface() call\")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Block 12: Complete Training Pipeline (Run All)\n",
        "\n",
        "def train_world_models_complete(config, use_wandb=False):\n",
        "    \"\"\"\n",
        "    Complete training pipeline for World Models\n",
        "    \n",
        "    This runs the entire training process:\n",
        "    1. Collect random episodes\n",
        "    2. Train VAE\n",
        "    3. Train MDN-RNN\n",
        "    4. Train Controller with CMA-ES\n",
        "    5. Evaluate and record videos\n",
        "    6. Optionally publish to Hugging Face\n",
        "    \"\"\"\n",
        "    \n",
        "    print(\"=\" * 70)\n",
        "    print(\"WORLD MODELS TRAINING PIPELINE\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    # Initialize WandB if requested\n",
        "    if use_wandb:\n",
        "        print(\"\\nInitializing Weights & Biases...\")\n",
        "        wandb_run = init_wandb()\n",
        "    \n",
        "    # Step 1: Collect Data\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"STEP 1: Collecting Random Episodes\")\n",
        "    print(\"=\" * 70)\n",
        "    episodes = collect_random_episodes(\n",
        "        config['env_name'],\n",
        "        num_episodes=config['num_random_episodes'],\n",
        "        max_steps=config['max_episode_steps']\n",
        "    )\n",
        "    \n",
        "    # Step 2: Train VAE\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"STEP 2: Training VAE (Vision)\")\n",
        "    print(\"=\" * 70)\n",
        "    vae = train_vae(episodes, config)\n",
        "    torch.save(vae.state_dict(), 'vae_model.pt')\n",
        "    \n",
        "    # Step 3: Train MDN-RNN\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"STEP 3: Training MDN-RNN (Memory)\")\n",
        "    print(\"=\" * 70)\n",
        "    rnn_sequences = prepare_rnn_data(episodes, vae, config)\n",
        "    rnn = train_mdnrnn(rnn_sequences, config)\n",
        "    torch.save(rnn.state_dict(), 'mdnrnn_model.pt')\n",
        "    \n",
        "    # Step 4: Train Controller\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"STEP 4: Training Controller (CMA-ES)\")\n",
        "    print(\"=\" * 70)\n",
        "    controller = train_controller_cmaes(vae, rnn, config)\n",
        "    torch.save(controller.state_dict(), 'controller_model.pt')\n",
        "    \n",
        "    # Step 5: Evaluate and Record\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"STEP 5: Evaluation and Video Recording\")\n",
        "    print(\"=\" * 70)\n",
        "    eval_stats = record_agent(\n",
        "        vae, rnn, controller,\n",
        "        config['env_name'],\n",
        "        num_episodes=config['eval_episodes']\n",
        "    )\n",
        "    \n",
        "    # Save evaluation results\n",
        "    with open('eval_results.json', 'w') as f:\n",
        "        json.dump(eval_stats, f, indent=2)\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"TRAINING COMPLETE!\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"Mean Reward: {eval_stats['mean_reward']:.2f}\")\n",
        "    print(f\"Models saved: vae_model.pt, mdnrnn_model.pt, controller_model.pt\")\n",
        "    print(f\"Videos saved in: ./videos/\")\n",
        "    \n",
        "    if use_wandb:\n",
        "        wandb.finish()\n",
        "    \n",
        "    return vae, rnn, controller, eval_stats\n",
        "\n",
        "# To run the complete pipeline, uncomment below:\n",
        "# vae, rnn, controller, stats = train_world_models_complete(config, use_wandb=False)\n",
        "\n",
        "print(\"\u2713 Complete pipeline ready\")\n",
        "print(\"\\nTo run full training:\")\n",
        "print(\"  vae, rnn, controller, stats = train_world_models_complete(config)\")\n",
        "print(\"\\nEstimated training time:\")\n",
        "print(\"  - VAE: ~10-20 minutes\")\n",
        "print(\"  - RNN: ~30-60 minutes\")\n",
        "print(\"  - Controller: ~2-4 hours (depends on num_generations)\")\n",
        "print(\"  - Total: ~3-5 hours on GPU\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2713 Complete pipeline ready\n",
            "\n",
            "To run full training:\n",
            "  vae, rnn, controller, stats = train_world_models_complete(config)\n",
            "\n",
            "Estimated training time:\n",
            "  - VAE: ~10-20 minutes\n",
            "  - RNN: ~30-60 minutes\n",
            "  - Controller: ~2-4 hours (depends on num_generations)\n",
            "  - Total: ~3-5 hours on GPU\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}